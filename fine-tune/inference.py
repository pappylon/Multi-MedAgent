import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '../'))

def load_local_model():
    print(f"â³ [System]: gpu_med_full_model ...")
    
    model_path = os.path.join(PROJECT_ROOT, "models", "gpu_med_full_model")

    # ====================================================
    # ğŸ§  æ™ºèƒ½è®¾å¤‡é€‰æ‹©é€»è¾‘ (å…³é”®ä¿®æ”¹)
    # ====================================================
    if torch.cuda.is_available():
        # æƒ…å†µ A: Windows ç”µè„‘ (æœ‰ NVIDIA æ˜¾å¡)
        device = "cuda"
        torch_dtype = torch.float16 # GPU ä¸Šç”¨ fp16 æ—¢å¿«åˆçœæ˜¾å­˜
        print("âœ… æ£€æµ‹åˆ° CUDA è®¾å¤‡ï¼Œå¯ç”¨ GPU åŠ é€Ÿæ¨¡å¼")
        
    elif torch.backends.mps.is_available():
        # æƒ…å†µ B: ä½ çš„ Mac ç”µè„‘
        # è™½ç„¶ Mac æœ‰ MPS åŠ é€Ÿï¼Œä½†å› ä¸ºæ¨¡å‹æ–‡ä»¶å¤ªå¤§ (14GB)ï¼Œä¼šå¯¼è‡´ Buffer æº¢å‡ºæŠ¥é”™
        # æ‰€ä»¥é’ˆå¯¹ Macï¼Œæˆ‘ä»¬å¼ºåˆ¶é™çº§åˆ° CPU
        device = "cpu" 
        torch_dtype = torch.float32 # CPU ç”¨ float32 å…¼å®¹æ€§æœ€å¥½
        print("âš ï¸ æ£€æµ‹åˆ° Mac MPSï¼Œä½†æ¨¡å‹è¿‡å¤§ (14GB+)ï¼Œå¼ºåˆ¶åˆ‡æ¢è‡³ CPU æ¨¡å¼ä»¥é¿å¼€ Metal é™åˆ¶ã€‚")
        print("ğŸ’¡ æç¤ºï¼šæœ¬åœ°æ¨ç†é€Ÿåº¦ä¼šè¾ƒæ…¢ï¼Œè¿™æ˜¯æ­£å¸¸çš„ã€‚")
        
    else:
        # æƒ…å†µ C: æ™®é€šç”µè„‘ (æ— æ˜¾å¡)
        device = "cpu"
        torch_dtype = torch.float32
        print("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU æ¨¡å¼ã€‚")

    try:
        # åŠ è½½ Tokenizer
        print(f"ğŸ“‚ åŠ è½½ Tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "right"

        # åŠ è½½æ¨¡å‹
        print(f"ğŸ“‚ åŠ è½½æ¨¡å‹æƒé‡...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map=device,           # æ™ºèƒ½åˆ†é… (Win->cuda, Mac->cpu)
            torch_dtype=torch_dtype,     # æ™ºèƒ½ç±»å‹ (Win->fp16, Mac->fp32)
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )

        print("âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return model, tokenizer, device

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None
    
def generate_local_response(model, tokenizer, device, prompt_text):
    """ç”Ÿæˆå›ç­” (å¢å¼ºç‰ˆ)"""
    inputs = tokenizer(prompt_text, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=True,
            temperature=0.3, # é™ä½æ¸©åº¦ï¼Œè®©é‡å†™æ›´ç¨³å®š
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # è§£ç 
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # âœ… å¢å¼ºè§£æé€»è¾‘
    # 1. å°è¯•æŒ‰æ ‡å‡†æ ¼å¼æˆªå–
    if "### Output:" in full_response:
        return full_response.split("### Output:")[-1].strip()
    
    # 2. å¦‚æœæ¨¡å‹æ²¡å†™ Output æ ‡ç­¾ï¼Œå°è¯•å»æ‰ Prompt æœ¬èº«
    # (æœ‰äº›æ¨¡å‹ä¼šæŠŠ Prompt å¤è¿°ä¸€é)
    if full_response.startswith(prompt_text):
        return full_response[len(prompt_text):].strip()
        
    # 3. å®åœ¨æ²¡åŠæ³•ï¼Œè¿”å›åŸæ¥çš„å…¨éƒ¨å†…å®¹ (æ€»æ¯”è¿”å›ç©ºå¥½)
    return full_response.strip()