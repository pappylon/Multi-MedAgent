import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '../'))

def load_local_model():
    print(f"â³ [System] : gpu_med_full_model ...")

    model_path = os.path.join(PROJECT_ROOT, "models", "gpu_med_full_model")

    # 1. ç¡¬ä»¶æ£€æµ‹ (é€‚é… Mac å’Œ Windows)
    if torch.cuda.is_available():
        device = "cuda"
        print("âœ… æ£€æµ‹åˆ° CUDA (NVIDIA GPU)")
    elif torch.backends.mps.is_available():
        device = "mps" # Mac M1/M2/M3 èŠ¯ç‰‡åŠ é€Ÿ
        print("âœ… æ£€æµ‹åˆ° MPS (Mac GPU)")
    else:
        device = "cpu"
        print("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPU æ¨¡å¼ (é€Ÿåº¦è¾ƒæ…¢)")

    try:
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶å¤¹ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {model_path}")

        # 2. åŠ è½½åˆ†è¯å™¨ (ç›´æ¥ä»æœ¬åœ°åŠ è½½)
        print(f"ğŸ“‚ åŠ è½½ Tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        # è¡¥å…¨è®¾ç½® (é˜²æ­¢æŠ¥é”™)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "right"

        # 3. åŠ è½½æ¨¡å‹ (ç›´æ¥åŠ è½½å®Œæ•´ç‰ˆ)
        print(f"ğŸ“‚ åŠ è½½æ¨¡å‹æƒé‡ (å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map=device,
            torch_dtype=torch.float16 if device != "cpu" else torch.float32,
            trust_remote_code=True
        )

        print("âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return model, tokenizer, device

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½ä¸¥é‡å¤±è´¥: {e}")
        # å¦‚æœåŠ è½½å¤±è´¥ï¼Œä¸ºäº†ä¸è®©ç¨‹åºå´©æºƒï¼Œæˆ‘ä»¬è¿”å› None
        return None, None, None

def generate_local_response(model, tokenizer, device, prompt_text):
    """ç”Ÿæˆå›ç­”"""
    # ç¡®ä¿è¾“å…¥æ•°æ®ä¹Ÿåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    inputs = tokenizer(prompt_text, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=300,  # ç¨å¾®è°ƒå¤§ä¸€ç‚¹ï¼Œå…è®¸å®ƒå¤šè¯´ç‚¹
            do_sample=True,      # å¯ç”¨é‡‡æ ·ï¼Œè®©å›ç­”æ›´è‡ªç„¶
            temperature=0.5,     # æ¸©åº¦ï¼šè¶Šä½è¶Šä¸¥è°¨ï¼Œè¶Šé«˜è¶Šæœ‰åˆ›é€ åŠ›
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # è§£ç 
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æ¸…æ´—æ•°æ®ï¼šåªä¿ç•™ ### Output: ä¹‹åçš„å†…å®¹
    if "### Output:" in full_response:
        return full_response.split("### Output:")[-1].strip()
    
    # å¦‚æœæ¨¡å‹æ²¡æœ‰ä¸¥æ ¼éµå®ˆ Output æ ¼å¼ï¼Œå°è¯•å»æ‰ Prompt éƒ¨åˆ†
    # (ç®€å•çš„å­—ç¬¦ä¸²å»é‡)
    if full_response.startswith(prompt_text):
        return full_response[len(prompt_text):].strip()

    return full_response