import os
import torch
from threading import Thread
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer, BitsAndBytesConfig
from peft import PeftModel

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '../'))

def load_local_model():
    print(f"â³ [System]: æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹è·¯å¾„...")
    
    # 1. å®šä¹‰è·¯å¾„
    # åŸºç¡€æ¨¡å‹è·¯å¾„
    base_model_path = os.path.join(PROJECT_ROOT, "models", "gpu_med_full_model")

    # LoRA é€‚é…å™¨è·¯å¾„
    adapter_path = os.path.join(base_model_path, "lora_medquad_1_epoch")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # å®šä¹‰ 4-bit é‡åŒ–é…ç½® (QLoRA æ ¸å¿ƒ)
    # è¿™èƒ½å¤§å¹…é™ä½æ˜¾å­˜å ç”¨ (16GB -> 6GB)ï¼Œå¹¶è§£å†³ OOM å´©æºƒé—®é¢˜
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16, # æ˜¾å¡å¦‚æœè¾ƒè€(å¦‚10ç³»åˆ—)ï¼Œå¯èƒ½éœ€è¦æ”¹ä¸º torch.float16
        bnb_4bit_use_double_quant=False,
    )

    try:
        # 2. åŠ è½½ Tokenizer
        print(f"ğŸ“‚ åŠ è½½ Tokenizer...")
        
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "right"
        # if 'llama' in MODEL_NAME.lower():
        # tokenizer.padding_side = "right"
        # tokenizer.pad_token = tokenizer.eos_token
        tokenizer.truncation_side = 'left'
        if tokenizer.chat_template and "generation" not in tokenizer.chat_template:
            tokenizer.chat_template(
                "{% set loop_messages = messages %}"
                "{% for message in loop_messages %}"
                "{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}"
                "{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}"
                "{% if message['role'] == 'assistant' %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}"
                "{% generation %}"
                "{{ message['content'] | trim + '<|eot_id|>' }}"
                "{% endgeneration %}"
                "{% else %}"
                "{{ content }}"
                "{% endif %}"
                "{% endfor %}"
                "{% if add_generation_prompt %}"
                "{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}"
                "{% endif %}"
            )

        # 3. åŠ è½½åŸºç¡€æ¨¡å‹ (åº”ç”¨ 4-bit é‡åŒ–)
        print(f"ğŸ“‚ åŠ è½½åŸºç¡€æ¨¡å‹ (4-bit Quantization)...")
        
        # Windows å…¼å®¹æ€§å¤„ç†ï¼š
        # å¦‚æœæ˜¯ CPU æ¨¡å¼ï¼Œä¸èƒ½ç”¨ 4-bit é‡åŒ–ï¼›å¦‚æœæ˜¯ GPUï¼Œå°è¯•åŠ è½½
        if device == "cuda":
            try:
                model = AutoModelForCausalLM.from_pretrained(
                    base_model_path,
                    quantization_config=bnb_config, # âœ… åº”ç”¨é˜Ÿå‹çš„é‡åŒ–é…ç½®
                    device_map="auto",              # è®© accelerate è‡ªåŠ¨åˆ†é…è®¾å¤‡
                    trust_remote_code=True
                )
            except ImportError:
                print("âš ï¸ æœªæ£€æµ‹åˆ° bitsandbytes åº“æˆ–ä¸æ”¯æŒ 4-bitï¼Œå›é€€åˆ° FP16 æ¨¡å¼...")
                model = AutoModelForCausalLM.from_pretrained(
                    base_model_path,
                    device_map="auto",
                    torch_dtype=torch.float16,
                    trust_remote_code=True
                )
        else:
            # CPU æ¨¡å¼
            print("âš ï¸ ä½¿ç”¨ CPU æ¨¡å¼ (é€Ÿåº¦è¾ƒæ…¢)...")
            model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                device_map="cpu",
                torch_dtype=torch.float32,
                trust_remote_code=True
            )

        # 4. åŠ è½½ LoRA å¾®è°ƒå‚æ•°
        if os.path.exists(adapter_path):
            print(f"ğŸ”— æ­£åœ¨æŒ‚è½½ LoRA å¾®è°ƒå‚æ•°: {os.path.basename(adapter_path)} ...")
            try:
                model = PeftModel.from_pretrained(model, adapter_path)
                print("âœ… LoRA å¾®è°ƒå‚æ•°åŠ è½½æˆåŠŸï¼(åŒ»ç–—æ¨¡å¼å·²æ¿€æ´»)")
            except Exception as e:
                print(f"âš ï¸ LoRA åŠ è½½æŠ¥é”™: {e}")
        else:
            print(f"\nâŒ [è­¦å‘Š] æ‰¾ä¸åˆ° LoRA è·¯å¾„: {adapter_path}ï¼Œå°†ä»…ä½¿ç”¨åŸºç¡€æ¨¡å‹ã€‚")

        return model, tokenizer, device

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None
    
def generate_local_response(model, tokenizer, device, formatted_prompt_text):
    """
    æ¥æ”¶å·²ç»å¡«å……å¥½çš„å®Œæ•´ Promptï¼Œæµå¼è¾“å‡ºåç»­å†…å®¹
    """
    try:
        # Llama-3 å®˜æ–¹æ ¼å¼å°è£… (å¯é€‰ï¼Œå–å†³äºä½ å¾®è°ƒæ—¶æœ‰æ²¡æœ‰åŠ è¿™ä¸ª)
        # å¦‚æœä½ å¾®è°ƒæ—¶ç›´æ¥ç”¨çš„ ### Instruction æ ¼å¼ï¼Œå¯ä»¥æŠŠä¸‹é¢è¿™è¡Œ f-string å»æ‰ï¼Œç›´æ¥ç”¨ formatted_prompt_text
        final_input = f"<start_of_turn>user\n{formatted_prompt_text}<end_of_turn>\n<start_of_turn>model\n"

        inputs = tokenizer(final_input, return_tensors="pt").to(model.device)
        
        # skip_prompt=True
        # å®ƒä¼šè‡ªåŠ¨è®¡ç®—è¾“å…¥æœ‰å¤šé•¿ï¼Œè¾“å‡ºæ—¶åªæ˜¾ç¤ºæ¨¡å‹æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        generation_kwargs = dict(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            streamer=streamer,
            max_new_tokens=128, 
            do_sample=True,
            temperature=0.2,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
        
        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()
        
        print("ğŸ¤– [AI]: ", end="", flush=True)
        
        full_response = ""
        

        for new_text in streamer:
            clean_text = new_text.replace("### Output:", "").replace("###", "").strip()
            
            if not clean_text:
                continue
                
            print(new_text, end="", flush=True) # æ‰“å°åŸå§‹æµå¼æ–‡æœ¬ä¿æŒæµç•…
            full_response += new_text

        print() # æ¢è¡Œ
        return full_response.strip()

    except Exception as e:
        print(f"Error: {e}")
        return ""

if __name__ == "__main__":
    m, t, d = load_local_model()
    if m:
        generate_local_response(m, t, d, "I have a headache")