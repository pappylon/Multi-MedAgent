import streamlit as st
import os
import sys
from dotenv import load_dotenv
from pathlib import Path

# --------------------------------
# è·¯å¾„è®¾ç½®
# --------------------------------
BASE_DIR = Path(__file__).resolve().parent.parent
SRC_DIR = BASE_DIR / "src"
sys.path.insert(0, str(SRC_DIR))


# from rag.engine import GeminiRAGEngine
from rag.engine import LocalLLMEngine


# --------------------------------
# åŠ è½½ç¯å¢ƒå˜é‡
# --------------------------------
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# --------------------------------
# é¡µé¢é…ç½®
# --------------------------------
st.set_page_config(page_title="Medical AI Agent (Local)", page_icon="ğŸ¥")
st.title("ğŸ¥ Medical AI Assistant")
# âœ… æ”¹åŠ¨ 2: æ›´æ–°å‰¯æ ‡é¢˜ï¼Œå¼ºè°ƒä½¿ç”¨äº†å¾®è°ƒæ¨¡å‹
st.caption("ğŸš€ Powered by **Local Fine-tuned Model** (Llama-3) & RAG Technology")

# --------------------------------
# åˆå§‹åŒ–å¼•æ“ (å¸¦ç¼“å­˜)
# --------------------------------
@st.cache_resource
def get_engine():
    # æ˜¾ç¤ºä¸€ä¸ªåŠ è½½è½¬åœˆåœˆï¼Œå› ä¸ºæœ¬åœ°åŠ è½½æ¯”è¾ƒæ…¢
    with st.spinner("æ­£åœ¨åŠ è½½æœ¬åœ°å¾®è°ƒæ¨¡å‹ (çº¦éœ€ 1-2 åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…)..."):
        try:
            # âœ… æ”¹åŠ¨ 3: å®ä¾‹åŒ–æœ¬åœ°å¼•æ“
            return LocalLLMEngine()
        except Exception as e:
            st.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None

# æ‰§è¡ŒåŠ è½½
engine = get_engine()

# --------------------------------
# Session State èŠå¤©å†å²
# --------------------------------
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --------------------------------
# å¤„ç†ç”¨æˆ·è¾“å…¥
# --------------------------------
if prompt := st.chat_input("Please describe your symptoms or concerns..."):
    # 1. æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})


    if engine:
        with st.chat_message("assistant"):
            placeholder = st.empty()
            placeholder.markdown("â³ *Local Model is thinking...*")

            try:

                history_for_engine = [
                    ("User" if m["role"] == "user" else "AI", m["content"])
                    for m in st.session_state.messages[:-1]
                ]

                response = engine.answer_question(prompt, history_for_engine)

                placeholder.markdown(response)
                st.session_state.messages.append(
                    {"role": "assistant", "content": response}
                )

            except Exception as e:
                placeholder.error(f"âŒRuntime error:{e}")
    else:
        st.error("âš ï¸ The engine has not been initialized and cannot respond. Please check the model path.")