import sys
import os

# è·¯å¾„ä¿®æ­£
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
# âœ… åŒæ ·å¼•å…¥å…è´¹çš„æœ¬åœ° Embedding
from langchain_huggingface import HuggingFaceEmbeddings 

# âœ… å¼•å…¥é…ç½® (ç¡®ä¿ src/config.py é‡Œæœ‰è¿™äº›å˜é‡)
# å¦‚æœä½  src/config.py é‡Œæ²¡æœ‰ OPENAI_API_BASEï¼Œè¯·è®°å¾—å»åŠ ä¸Š
from src.config import CHROMA_PATH, OPENAI_API_KEY, OPENAI_API_BASE

def main():
    print("ğŸš€ æ­£åœ¨å¯åŠ¨åŒ»ç–— Agent...")

    # 1. å‡†å¤‡ Embedding (å¿…é¡»å’Œ build_db.py ç”¨åŒä¸€ä¸ªæ¨¡å‹)
    print("Loading embeddings...")
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
    if not os.path.exists(CHROMA_PATH):
        print("âŒ é”™è¯¯: æ•°æ®åº“ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ python scripts/build_db.py")
        return
        
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)
    
    # 2. å‡†å¤‡å¤§è„‘ (LLM)
    # è¿™é‡Œé…ç½®äº† base_urlï¼Œæ‰€ä»¥æ—¢æ”¯æŒ OpenAIï¼Œä¹Ÿæ”¯æŒ DeepSeek
    print(f"Connecting to LLM (Base URL: {OPENAI_API_BASE})...")
    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo", # å¦‚æœç”¨ DeepSeekï¼Œå¯ä»¥æ”¹æˆ "deepseek-chat"
        temperature=0,
        openai_api_key=OPENAI_API_KEY,
        base_url=OPENAI_API_BASE 
    )
    
    # 3. å‡†å¤‡é—®ç­”é“¾
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=db.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True
    )
    
    # 4. å¼€å§‹äº¤äº’
    print("\nâœ… ç³»ç»Ÿå°±ç»ªï¼æˆ‘æ˜¯ä½ çš„å…¨ç§‘åŒ»ç–—åŠ©æ‰‹ã€‚")
    print("(è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º)")
    
    while True:
        try:
            query = input("\nğŸ‘¨â€âš•ï¸ è¯·æè¿°ç—‡çŠ¶: ")
            if query.lower() in ['quit', 'exit']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            if not query.strip():
                continue
                
            print("ğŸ¤” æ€è€ƒä¸­...", end="", flush=True)
            result = qa_chain.invoke({"query": query})
            print("\r" + " " * 20 + "\r", end="") # æ¸…é™¤"æ€è€ƒä¸­"
            
            print(f"ğŸ¤– AI å»ºè®®: \n{result['result']}")
            
            # æ‰“å°å‚è€ƒæ¥æº (å¯é€‰)
            # print("\nğŸ“š å‚è€ƒæ–‡æ¡£:")
            # for doc in result['source_documents']:
            #     source = os.path.basename(doc.metadata.get('source', 'unknown'))
            #     print(f"- {source} (Page {doc.metadata.get('page', 0)})")

        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()