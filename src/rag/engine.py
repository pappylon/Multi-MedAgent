import sys
import os
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI, HarmBlockThreshold, HarmCategory

# å¯¼å…¥é…ç½®å’ŒåŠ è½½å™¨
from rag.loader import VectorDBLoader
from rag.config import MEDICAL_PROMPT_TEMPLATE, REWRITE_PROMPT_TEMPLATE

# ========================================================
# 1. åŠ¨æ€æ·»åŠ  fine-tune è·¯å¾„
# ========================================================
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, "../../"))
fine_tune_dir = os.path.join(project_root, "fine-tune")

if fine_tune_dir not in sys.path:
    sys.path.append(fine_tune_dir)

try:
    from inference import load_local_model, generate_local_response
    LOCAL_MODEL_AVAILABLE = True
except ImportError:
    LOCAL_MODEL_AVAILABLE = False

# ========================================================
# 2. GeminiRAGEngine
# ========================================================
class GeminiRAGEngine:
    def __init__(self, google_api_key: str, k: int = 5, temperature: float = 0):
        # 1. æ£€ç´¢å™¨
        print("ğŸ” åˆå§‹åŒ– Gemini RAG æ£€ç´¢å™¨...")
        loader = VectorDBLoader(k=k)
        self.retriever = loader.load_db()

        # 2. LLM åˆå§‹åŒ–
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            google_api_key=google_api_key,
            temperature=temperature,
            safety_settings={
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            }
        )
        
        # 3. å›ç­”æ¨¡æ¿
        self.prompt = PromptTemplate(
            template=MEDICAL_PROMPT_TEMPLATE,
            input_variables=["context", "chat_history", "question"]
        )
        
        # 4. âœ… å…³é”®ä¿®å¤ï¼šåŠ è½½é‡å†™æ¨¡æ¿ (LangChain é“¾å¼è°ƒç”¨)
        # è¿™ä¼šè®© Gemini å…ˆæŠŠ "it" ç¿»è¯‘æˆ "headache"
        rewrite_prompt_template = PromptTemplate(
            template=REWRITE_PROMPT_TEMPLATE,
            input_variables=["chat_history", "question"]
        )
        # æ„é€ é“¾ï¼šPrompt -> LLM -> String
        self.rewrite_chain = rewrite_prompt_template | self.llm | StrOutputParser()

    def rewrite_query(self, question: str, chat_history: list) -> str:
        """åˆ©ç”¨ Gemini è‡ªèº«çš„é«˜æ™ºå•†æ¥é‡å†™é—®é¢˜"""
        if not chat_history:
            return question
            
        # æ ¼å¼åŒ–å†å²è®°å½•
        history_text = "\n".join([f"{role}: {text}" for role, text in chat_history[-3:]])
        
        try:
            # æ‰§è¡Œé‡å†™
            new_q = self.rewrite_chain.invoke({
                "chat_history": history_text,
                "question": question
            })
            # æ‰“å°å‡ºæ¥ï¼Œè®©ä½ ç¡®è®¤å®ƒæ˜¯å¦å·¥ä½œ
            # print(f"ğŸ”„ [Gemini Rewrite] '{question}' -> '{new_q.strip()}'")
            return new_q.strip()
        except Exception as e:
            print(f"âš ï¸ é‡å†™å¤±è´¥: {e}")
            return question

    def answer_question(self, question: str, chat_history: list = None) -> str:
        # 1. ç¬¬ä¸€æ­¥ï¼šé‡å†™ (Rewriting)
        # "How to solve it?" -> "How to solve headache?"
        search_query = self.rewrite_query(question, chat_history)
        
        # 2. ç¬¬äºŒæ­¥ï¼šæ£€ç´¢ (Retrieval)
        # ç”¨ "headache" å»æœï¼Œè‚¯å®šèƒ½æœåˆ°
        print(f"ğŸ” [Gemini RAG] æ­£åœ¨æ£€ç´¢: {search_query}")
        docs = self.retriever.invoke(search_query)

        # ==================== ğŸ› ï¸ Debug ä»£ç å¼€å§‹ ====================
        print(f"ğŸ“„ [Debug] æ£€ç´¢åˆ°äº† {len(docs)} ä¸ªç›¸å…³ç‰‡æ®µ")
        if len(docs) > 0:
            print(f"ğŸ“„ [Debug] ç‰‡æ®µ 1 é¢„è§ˆ: {docs[0].page_content[:100]}...") # æ‰“å°å‰100ä¸ªå­—çœ‹çœ‹
            # print(f"ğŸ“„ [Debug] æ¥æº: {docs[0].metadata}")
        else:
            print("âš ï¸ [Debug] è­¦å‘Šï¼šæ²¡æœ‰æ£€ç´¢åˆ°ä»»ä½•æ–‡æ¡£ï¼Context ä¸ºç©ºï¼")
        # ==================== ğŸ› ï¸ Debug ä»£ç ç»“æŸ ====================
        
        context_text = "\n\n---\n\n".join([d.page_content for d in docs]) if docs else ""
        
        # 3. ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆ (Generation)
        history_text = ""
        if chat_history:
            for role, text in chat_history[-6:]:
                history_text += f"{role}: {text}\n"

        full_prompt = self.prompt.format(
            context=context_text, 
            chat_history=history_text, 
            question=question
        )
        
        print("ğŸ§  Gemini æ­£åœ¨ç”Ÿæˆå›ç­”...")
        resp = self.llm.invoke(full_prompt)
        return getattr(resp, "content", str(resp))


# ========================================================
# 3. LocalRAGEngine
# ========================================================
class LocalRAGEngine:
    def __init__(self, k: int = 3):
        if not LOCAL_MODEL_AVAILABLE:
            raise ImportError("inference.py æœªæ‰¾åˆ°")
        loader = VectorDBLoader(k=k)
        self.retriever = loader.load_db()
        self.model, self.tokenizer, self.device = load_local_model()
        
        # å³ä½¿æ˜¯ Local å¼•æ“ï¼Œå¦‚æœç¯å¢ƒå…è®¸ï¼Œæœ€å¥½ä¹Ÿç”¨ Gemini æ¥é‡å†™(æ›´å‡†)ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
        self.prompt = PromptTemplate(template=MEDICAL_PROMPT_TEMPLATE, input_variables=["context", "chat_history", "question"])

    def answer_question(self, question: str, chat_history: list = None) -> str:
        # ç®€åŒ–çš„ Local é€»è¾‘
        # çœŸæ­£è·‘çš„æ—¶å€™å»ºè®®å‚è€ƒä¸Šä¸€æ¡å›ç­”çš„ Hybrid å†™æ³•
        docs = self.retriever.invoke(question)
        context_text = "\n".join([d.page_content for d in docs]) if docs else ""
        history_text = "\n".join([f"{r}: {t}" for r,t in (chat_history or [])[-2:]])
        full_prompt = self.prompt.format(context=context_text, chat_history=history_text, question=question)
        response = generate_local_response(self.model, self.tokenizer, self.device, full_prompt)
        return response