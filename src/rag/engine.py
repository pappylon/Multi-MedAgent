import sys
import os
import inspect # ç”¨äºæ‰“å°æ¨¡å—è·¯å¾„
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI, HarmBlockThreshold, HarmCategory

# å¯¼å…¥é…ç½®å’ŒåŠ è½½å™¨
from rag.loader import VectorDBLoader
from rag.config import MEDICAL_PROMPT_TEMPLATE, REWRITE_PROMPT_TEMPLATE, DIRECT_CHAT_TEMPLATE

# ========================================================
# 1. å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶ä¼˜å…ˆåŠ è½½ fine-tune è·¯å¾„
# ========================================================
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, "../../"))
fine_tune_dir = os.path.join(project_root, "fine-tune")

# ğŸ”´ å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ insert(0) è€Œä¸æ˜¯ append
# è¿™ç¡®ä¿ Python ç¬¬ä¸€ä¸ªå» fine-tune æ–‡ä»¶å¤¹æ‰¾ inference.py
if fine_tune_dir not in sys.path:
    sys.path.insert(0, fine_tune_dir)

try:
    import inference # å…ˆå¯¼å…¥æ¨¡å—
    from inference import load_local_model, generate_local_response
    
    # ğŸ•µï¸â€â™‚ï¸ Debug: æ‰“å°åˆ°åº•åŠ è½½äº†å“ªé‡Œçš„ inference æ–‡ä»¶
    print(f"âœ… æˆåŠŸå¯¼å…¥ inference æ¨¡å—")
    print(f"   ğŸ“‚ æ¥æºè·¯å¾„: {os.path.abspath(inference.__file__)}")
    
    # äºŒæ¬¡æ£€æŸ¥ï¼šç¡®ä¿å‡½æ•°ä¸æ˜¯ None
    if generate_local_response is None:
        raise ImportError("generate_local_response å‡½æ•°ä¸ºç©º (None)ï¼")
        
    LOCAL_MODEL_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ æ— æ³•å¯¼å…¥ inference æ¨¡å—: {e}")
    print(f"   ğŸ‘€ å½“å‰ sys.path: {sys.path}")
    LOCAL_MODEL_AVAILABLE = False
except Exception as e:
    print(f"âš ï¸ å¯¼å…¥ inference æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    LOCAL_MODEL_AVAILABLE = False

# ========================================================
# 2. GeminiRAGEngine
# ========================================================
class GeminiRAGEngine:
    def __init__(self, google_api_key: str, k: int = 5, temperature: float = 0):
        print("ğŸ” [Gemini] åˆå§‹åŒ– RAG æ£€ç´¢å™¨...")
        loader = VectorDBLoader(k=k)
        self.retriever = loader.load_db()

        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            google_api_key=google_api_key,
            temperature=temperature,
            safety_settings={
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            }
        )
        
        self.prompt = PromptTemplate(
            template=MEDICAL_PROMPT_TEMPLATE,
            input_variables=["context", "chat_history", "question"]
        )
        
        rewrite_prompt_template = PromptTemplate(
            template=REWRITE_PROMPT_TEMPLATE,
            input_variables=["chat_history", "question"]
        )
        self.rewrite_chain = rewrite_prompt_template | self.llm | StrOutputParser()

    def rewrite_query(self, question: str, chat_history: list) -> str:
        if not chat_history:
            return question
        history_text = "\n".join([f"{role}: {text}" for role, text in chat_history[-3:]])
        try:
            new_q = self.rewrite_chain.invoke({"chat_history": history_text, "question": question})
            return new_q.strip()
        except Exception as e:
            print(f"âš ï¸ [Gemini] é‡å†™å¤±è´¥: {e}")
            return question

    def answer_question(self, question: str, chat_history: list = None) -> str:
        search_query = self.rewrite_query(question, chat_history)
        print(f"ğŸ” [Gemini] æ­£åœ¨æ£€ç´¢: {search_query}")
        docs = self.retriever.invoke(search_query)
        print(f"ğŸ“„ [Gemini] æ£€ç´¢åˆ°äº† {len(docs)} ä¸ªç›¸å…³ç‰‡æ®µ")
        
        context_text = "\n\n---\n\n".join([d.page_content for d in docs]) if docs else ""
        history_text = ""
        if chat_history:
            for role, text in chat_history[-6:]:
                history_text += f"{role}: {text}\n"

        full_prompt = self.prompt.format(context=context_text, chat_history=history_text, question=question)
        print("ğŸ§  [Gemini] æ­£åœ¨ç”Ÿæˆå›ç­”...")
        resp = self.llm.invoke(full_prompt)
        return getattr(resp, "content", str(resp))


# ========================================================
# 3. LocalRAGEngine
# ========================================================
class LocalRAGEngine:
    def __init__(self, k: int = 5):
        if not LOCAL_MODEL_AVAILABLE:
            raise ImportError("inference.py not found. Please check the fine-tune directory.")
            
        print("Initializing RAG Retriever......")
        # loader = VectorDBLoader(k=k)
        # self.retriever = loader.load_db()
        
        print("Load local model...")
        self.model, self.tokenizer, self.device = load_local_model()
        
        self.prompt = PromptTemplate(
            template=MEDICAL_PROMPT_TEMPLATE, 
            input_variables=["context", "chat_history", "question"]
        )
        self.rewrite_prompt = PromptTemplate(
            template=REWRITE_PROMPT_TEMPLATE, 
            input_variables=["chat_history", "question"]
        )

    def rewrite_query(self, question: str, chat_history: list) -> str:
        """Rewriting Issues Using Local Models"""
        if not chat_history:
            return question
        
        history_text = "\n".join([f"{role}: {text}" for role, text in chat_history[-3:]])
        full_rewrite_prompt = self.rewrite_prompt.format(chat_history=history_text, question=question)
        
        try:
            print(f"Rewriting the question...")

            new_q = generate_local_response(self.model, self.tokenizer, self.device, full_rewrite_prompt)
            return new_q.strip().split('\n')[0]
        except Exception as e:
            print(f"Rewrite failed: {e}")
            return question

    def answer_question(self, question: str, chat_history: list = None) -> str:
        search_query = self.rewrite_query(question, chat_history)
        
        docs = self.retriever.invoke(search_query)
        
        context_text = "\n\n---\n\n".join([d.page_content for d in docs]) if docs else ""
        
        history_text = ""
        if chat_history:
            for role, text in chat_history[-6:]:
                history_text += f"{role}: {text}\n"

        full_prompt = self.prompt.format(
            context=context_text, 
            chat_history=history_text, 
            question=question
        )
        

        if generate_local_response is None:
            return "Internal error: The generate_local_response function has not been loaded."
            
        response = generate_local_response(self.model, self.tokenizer, self.device, full_prompt)
        return response
    
class LocalLLMEngine:
    def __init__(self):
        if not LOCAL_MODEL_AVAILABLE:
            raise ImportError("inference.py not found. Please check the fine-tune directory.")
            
        print("ğŸš€ [LocalLLM] Initializing Direct Local Model (No RAG)...")
        
        # âŒ ä¸åŠ è½½ VectorDBLoader
        # self.retriever = ... (ä¸éœ€è¦)
        
        print("Load local model...")
        # å¤ç”¨ inference.py é‡Œçš„åŠ è½½å‡½æ•°
        self.model, self.tokenizer, self.device = load_local_model()
        
        # ä½¿ç”¨çº¯å¯¹è¯æ¨¡æ¿
        self.prompt = PromptTemplate(
            template=DIRECT_CHAT_TEMPLATE, 
            input_variables=["chat_history", "question"]
        )


    def answer_question(self, question: str, chat_history: list = None) -> str:
        """
        ç›´æ¥è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œå›ç­”ï¼Œä¸è¿›è¡Œæ£€ç´¢
        """
        # 1. æ ¼å¼åŒ–å†å²è®°å½•
        history_text = "None"
        total_input = []
        total_input.append({"role": "system", "content": "You are a helpful and professional medical assistant. "
        "Answer the user's question based on your internal knowledge. "
        "Be concise, safe, and empathetic"
        })
        history_list = []
        if chat_history:
            # æ‹¼æ¥æœ€è¿‘ 6 æ¡è®°å½•
            history_text = ""
            for role, text in chat_history[-6:]:
                history_text += f"{role}: {text}\n"
                history_list.append({"role": role, "content":text})

        # total_input.extend(history_list)
        # total_input.append({"role": "user", "content": question})
        # total_input.extend(history_list)
        # 2. å¡«å…… Prompt (æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦ context å‚æ•°äº†)
        full_prompt = self.prompt.format(
            chat_history=history_text, 
            question=question
        )
        
        print(f"ğŸ¤– [LocalLLM] Generating response for: {question}")

        print("\n*****************" + full_prompt + "******************\n")


        # 3. æ£€æŸ¥ç”Ÿæˆå‡½æ•°æ˜¯å¦å­˜åœ¨
        if generate_local_response is None:
            return "Internal error: The generate_local_response function has not been loaded."
            
        # 4. è°ƒç”¨ inference.py è¿›è¡Œç”Ÿæˆ
        # æ³¨æ„ï¼šgenerate_local_response ä¼šè‡ªåŠ¨åŠ ä¸Š <start_of_turn>user ç­‰æ ‡ç­¾
        response = generate_local_response(self.model, self.tokenizer, self.device, full_prompt)
        
        return response